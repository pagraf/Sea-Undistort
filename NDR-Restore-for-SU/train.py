import os
import math
import argparse
import random
import logging
import sys

import torch
import torch.distributed as dist
# import torch.multiprocessing as mp

import options.options as option
from utils import util
from data import create_dataloader, create_dataset
from models import create_model
import numpy as np
import cv2

# Sea-Undistort Dataset
sys.path.append('../Sea-Undistort-Dataloader') # IMPORTANT: Change to the correct path
from SeaUndistort import SeaUndistort


def compute_ssim(img1, img2):

    ssims = []
    for i in range(3):
        ssims.append(_ssim(img1[i], img2[i]))
    return np.array(ssims).mean()


def _ssim(img1, img2):
    """Calculate SSIM (structural similarity) for one channel images.

    It is called by func:`calculate_ssim`.

    Args:
        img1 (ndarray): Images with range [0, 255] with order 'HWC'.
        img2 (ndarray): Images with range [0, 255] with order 'HWC'.

    Returns:
        float: ssim result.
    """

    C1 = (0.01 * 255)**2
    C2 = (0.03 * 255)**2

    img1 = img1.astype(np.float64)
    img2 = img2.astype(np.float64)
    kernel = cv2.getGaussianKernel(11, 1.5)
    window = np.outer(kernel, kernel.transpose())

    mu1 = cv2.filter2D(img1, -1, window)[5:-5, 5:-5]
    mu2 = cv2.filter2D(img2, -1, window)[5:-5, 5:-5]
    mu1_sq = mu1**2
    mu2_sq = mu2**2
    mu1_mu2 = mu1 * mu2
    sigma1_sq = cv2.filter2D(img1**2, -1, window)[5:-5, 5:-5] - mu1_sq
    sigma2_sq = cv2.filter2D(img2**2, -1, window)[5:-5, 5:-5] - mu2_sq
    sigma12 = cv2.filter2D(img1 * img2, -1, window)[5:-5, 5:-5] - mu1_mu2

    ssim_map = ((2 * mu1_mu2 + C1) *
                (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) *
                                       (sigma1_sq + sigma2_sq + C2))
    return ssim_map.mean()


def main():
    #### options
    parser = argparse.ArgumentParser()
    parser.add_argument('-opt', type=str, help='Path to option YMAL file.')
    parser.add_argument('--launcher', choices=['none', 'pytorch'], default='none',
                        help='job launcher')
    parser.add_argument('--local_rank', type=int, default=0)
    args = parser.parse_args()
    opt = option.parse(args.opt, is_train=True)


    #### loading resume state if exists
    if opt['path'].get('resume_state', None):
        # distributed resuming: all load into default GPU
        device_id = torch.cuda.current_device()
        resume_state = torch.load(opt['path']['resume_state'],
                                  map_location=lambda storage, loc: storage.cuda(device_id))
        option.check_resume(opt, resume_state['iter'])  # check resume options
    else:
        resume_state = None

    #### mkdir and loggers
    if resume_state is None:
        util.mkdir_and_rename(
            opt['path']['experiments_root'])  # rename experiment folder if exists
        util.mkdirs((path for key, path in opt['path'].items() if not key == 'experiments_root'
                        and 'pretrain_model' not in key and 'resume' not in key))

    # config loggers. Before it, the log will not work
    util.setup_logger('base', opt['path']['log'], 'train_' + opt['name'], level=logging.INFO,
                        screen=True, tofile=True)
    util.setup_logger('val', opt['path']['log'], 'val_' + opt['name'], level=logging.INFO,
                        screen=True, tofile=True)
    logger = logging.getLogger('base')
    logger.info(option.dict2str(opt))
       


    # convert to NoneDict, which returns None for missing keys
    opt = option.dict_to_nonedict(opt)

    #### random seed
    seed = opt['train']['manual_seed']
    if seed is None:
        seed = random.randint(1, 10000)
    
    logger.info('Random seed: {}'.format(seed))
    util.set_random_seed(seed)

    torch.backends.cudnn.benchmark = True
    # torch.backends.cudnn.deterministic = True

    #### create train and val dataloader
    best_psnr = 0

    bathy = True
    if bathy:
        for phase, dataset_opt in opt['datasets'].items():
            if phase == 'train':
                # train_set = create_dataset(dataset_opt)
                train_set = SeaUndistort(dataset_opt['params'], split_mode='train')
                logger.info('Number of images in {:s} data set: {:d}'.format(phase, len(train_set)))
                train_size = int(math.ceil(len(train_set) / dataset_opt['batch_size']))

                total_iters = int(opt['train']['niter'])
                total_epochs = int(math.ceil(total_iters / train_size))
                
                train_sampler = None
                train_loader = create_dataloader(train_set, dataset_opt, opt, train_sampler)
                
                logger.info('Number of train images: {:,d}, iters: {:,d}'.format(
                    len(train_set), train_size))
                logger.info('Total epochs needed: {:d} for iters {:,d}'.format(
                    total_epochs, total_iters))
            elif phase == 'val':
                # val_set = create_dataset(dataset_opt)
                val_set = SeaUndistort(dataset_opt['params'], split_mode='val')
                logger.info('Number of images in {:s} data set: {:d}'.format(phase, len(val_set)))
                val_loader = create_dataloader(val_set, dataset_opt, opt, None)
                logger.info('Number of val images in [{:s}]: {:d}'.format(
                    dataset_opt['name'], len(val_set)))
            else:
                raise NotImplementedError('Phase [{:s}] is not recognized.'.format(phase))
    else:
        for phase, dataset_opt in opt['datasets'].items():
            if phase == 'train':
                train_set = create_dataset(dataset_opt)
                train_size = int(math.ceil(len(train_set) / dataset_opt['batch_size']))

                total_iters = int(opt['train']['niter'])
                total_epochs = int(math.ceil(total_iters / train_size))
                
                train_sampler = None
                train_loader = create_dataloader(train_set, dataset_opt, opt, train_sampler)
                
                logger.info('Number of train images: {:,d}, iters: {:,d}'.format(
                    len(train_set), train_size))
                logger.info('Total epochs needed: {:d} for iters {:,d}'.format(
                    total_epochs, total_iters))
            elif phase == 'val':
                val_set = create_dataset(dataset_opt)
                val_loader = create_dataloader(val_set, dataset_opt, opt, None)
                logger.info('Number of val images in [{:s}]: {:d}'.format(
                    dataset_opt['name'], len(val_set)))
            else:
                raise NotImplementedError('Phase [{:s}] is not recognized.'.format(phase))
    assert train_loader is not None

    #### create model
    model = create_model(opt)

    #### resume training
    if resume_state:
        logger.info('Resuming training from epoch: {}, iter: {}.'.format(
            resume_state['epoch'], resume_state['iter']))

        start_epoch = resume_state['epoch']
        current_step = resume_state['iter']
        model.resume_training(resume_state)  # handle optimizers and schedulers
    else:
        current_step = 0
        start_epoch = 0

    val_freq = opt['train']['val_freq']
    #### training
    logger.info('Start training from epoch: {:d}, iter: {:d}'.format(start_epoch, current_step))
    for epoch in range(start_epoch, total_epochs + 1):
        if opt['dist']:
            train_sampler.set_epoch(epoch)
        for _, train_data in enumerate(train_loader):
            current_step += 1

            if current_step > total_iters:
                break
            #### training
            model.feed_data(train_data)
            model.optimize_parameters(current_step)

            #### update learning rate
            model.update_learning_rate(current_step, warmup_iter=opt['train']['warmup_iter'])

            #### log
            if current_step % opt['logger']['print_freq'] == 0:
                logs = model.get_current_log()
                message = '<epoch:{:3d}, iter:{:8,d}, lr:{:.3e}> '.format(
                    epoch, current_step, model.get_current_learning_rate())
                for k, v in logs.items():
                    message += '{:s}: {:.4e} '.format(k, v)
                
                logger.info(message)

            # validation
            if current_step % val_freq == 0:
                avg_psnr = 0.0
                avg_ssim = 0.0
                idx = 0
                for val_data in val_loader:
                    idx += 1
                    model.feed_data_test(val_data)
                    model.test()
                    visuals = model.get_current_visuals()
                    out_img = visuals['out_img'].numpy()
                    gt_img = visuals['gt_img'].numpy()

                    c, h, w = gt_img.shape
                    out_img = out_img[:c, :h, :w]

                    def compute_psnr(img_orig, img_out, peak):
                        mse = np.mean(np.square(img_orig - img_out))
                        psnr = 10 * np.log10(peak * peak / mse)
                        return psnr

                    curr_psnr = compute_psnr(out_img, gt_img, 1)
                    curr_ssim = compute_ssim(out_img * 255, gt_img * 255)

                    avg_psnr += curr_psnr
                    avg_ssim += curr_ssim

                avg_psnr = avg_psnr / idx
                avg_ssim = avg_ssim / idx

                logger_val = logging.getLogger('val')  # validation logger
                logger_val.info(
                    '<epoch:{:3d}, iter:{:8,d}> PSNR: {:.4f} SSIM: {:.4f}'.format( epoch, current_step, avg_psnr, avg_ssim))
                # ————————————————————————————————————————————————————————————————————
            #### save models and training states
            if current_step % opt['logger']['save_checkpoint_freq'] == 0:
                logger.info('Saving models and training states.')
                model.save(current_step)
                model.save_training_state(epoch, current_step)

    
    logger.info('Saving the final model.')
    model.save('latest')
    logger.info('End of training.')


if __name__ == '__main__':
    main()
